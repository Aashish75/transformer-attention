This project contains the following- 

-A loose implementation of the **attention mechanism**, from the "Attention Is All You Need" paper

-**Sequence to sequence** translation using a **transformer**

-**Visualising** the encoder, decoder and cross attention using **bertviz**. 

The pipeline followed is - 

1. Prepare the data.
2. Get the embeddings of the training partition.
3. Write the elemnents of the self-attention mechanism.
4. Train a transformer model
5. Evaluate the model.
6. Visualize the attention mechanism.
